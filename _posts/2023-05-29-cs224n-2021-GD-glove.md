---
layout: post
title: CS224N 2021 GD 和 GloVe
date: 2023-05-29 15:18 +0000
categories: [课程笔记, CS224N]
tags: [nlp, 基础]
math: true
---

首先简要介绍优化算法：梯度下降（GD）和随机梯度下降（SGD），其次介绍Glove模型。

## 梯度下降（Gradient Descent）

常见的梯度下降法包括随机梯度下降法（SGD）、批梯度下降法、Momentum 梯度下降法、Nesterov Momentum 梯度下降法、AdaGrad 梯度下降法、RMSprop 梯度下降法、Adam 梯度下降法。

下面以最简单的线性模型举例：$h(\theta)=\theta_0+\theta_1x_1+\theta_2x_2+\ldots+\theta_ix_i$

假设损失函数为：$J(\theta)=\frac{1}{2}[h_t(x)-y_t]^2$

那么梯度下降的基本形式就是：$\theta_{t+1}=\theta_t-\alpha J'(\theta)$

其中，$\alpha$ 是学习率。而 $J'(\theta)=[h_{\theta}(x)-y]\cdot h_{\theta}'$

其中 $h_{\theta}'=x$（因为这里假设的是线性函数！）

所以，$\theta_{t+1}=\theta_t-\alpha[h_{\theta}(x)-y]\cdot x$

下面都将以 h(x) 为线性函数举例。

### 标准梯度下降（GD）

标准梯度下降法（Gradient Descent，GD）的每次迭代**使用所有样本来计算梯度**，然后更新参数。标准梯度下降法的优点是收敛性能好，缺点是计算速度慢，尤其是在样本量很大时，每次迭代都要使用所有样本来计算梯度，计算量非常大。

$$
\theta_{t+1} = \theta_t - \alpha J'(\theta) = \theta_t - \frac{\alpha}{n}\sum_{i=1}^n[h_{\theta}(x^{(i)})-y^{(i)}]\cdot x^{(i)}
$$

### 随机梯度下降（SGD）

随机梯度下降法（Stochastic Gradient Descent，SGD）是**每次迭代只使用一个样本（每次随机从样本集中抽取一个样本，而不是对所有样本）**来计算梯度，然后更新参数。SGD 的优点是计算速度快，缺点是收敛性能不稳定，可能会在最优点附近震荡，甚至无法收敛。因此学习率 $\alpha$ 不能设置过大（当然也不能设置过小，否则收敛速度会过慢）。

从 n 个样本 $x^(1),x^(2),\ldots,x^(n)$ 中随机选一个 $x^(i)$ 进行梯度计算，更新公式为：

$$
\theta_{t+1} = \theta_t - \alpha J'(\theta) = \theta_t - \alpha[h_{\theta}(x^{(i)})-y^{(i)}]\cdot x^{(i)}
$$

### 批梯度下降（Batch Gradient Descent）

批梯度下降法（Batch Gradient Descent，BGD）是每次迭代使用 batch_size 个样本（从所有样本中抽取一个 batch 大小的样本数）来计算梯度，然后更新参数。

$$
\theta_{t+1} = \theta_t - \alpha J'(\theta) = \theta_t - \frac{\alpha}{batch\_size}\sum_{i=1}^{batch\_size}[h_{\theta}(x^{(i)})-y^{(i)}]\cdot x^{(i)}
$$

### Momentum 梯度下降

Momentum 梯度下降法（Momentum Gradient Descent）是在标准梯度下降法的基础上增加了一个动量项，用来加速收敛。**动量项是前几次梯度的累加，可以理解为一个积累效应，可以在一定程度上减少梯度的震荡**。

基本思想是：损失函数求最优解的过程可以看作小球从求解面（损失函数值在坐标系中呈现出的面）某处沿着该面下落直至该面最低处的过程，损失函数的梯度可以看作是施加给小球的力，通过力的作用有了速度，通过速度可以改变小球的位置。由力学定律 $F=ma$ 可知梯度与加速度成正比，加速度改变速度，便可以得到以下更新过程：

$$
v_{t+1} = \gamma v_t - \alpha J'(\theta)
$$

$$
\theta_{t+1} = \theta_t + v_{t+1}
$$

其中，$\gamma$ 是动量因子，通常设为 0.9。

这种优化方式不会立刻改变梯度优化的方向，而是每一次计算后的方向为前一次优化的方向与本次计算得到的值的加权和，即梯度优化的方向是通过积累一点点改变的，而且越积累越大。这种优化方式的好处在于通过不同训练样本求得梯度时，**始终都会增大最优方向上的梯度值**，因此可以减少许多震荡。

### Nesterov Momentum 梯度下降

Nesterov Momentum 梯度下降法（Nesterov Momentum Gradient Descent）是在 Momentum 梯度下降法的基础上进行改进，**在计算梯度之前，先根据上一次的动量方向更新参数，然后再计算梯度**。

$$
v_{t+1} = \gamma v_t - \alpha J'(\theta + \gamma v_t)
$$

$$
\theta_{t+1} = \theta_t + v_{t+1}
$$

Nesterov Momentum 相比于 Momentum 的区别在于，Nesterov Momentum 先**根据上一次的动量方向**更新参数，然后再计算梯度，而 Momentum 先计算梯度，然后再根据梯度更新参数。也就是说，Nesterov Momentum 会「向前看一步」，不是计算当前位置 $\theta$ 的梯度，而是计算 $\theta + \gamma v_t$ 的梯度，然后根据这个梯度来更新参数。

### Adagrad 梯度下降

**在梯度下降法中，学习率的选取十分重要**，因为它关乎到优化过程每一步“迈的大小”，迈的太大会导致在最优解附近来回震荡，迈的太小容易陷入局部最优解；且不同参数适用的学习率的大小不一样，有些参数可能已经接近最优，仅仅需要微调，需要比较小的学习率；而有些参数还需要大幅度调动。在这种场景下，**AdaGrad 能够自适应不同的学习率**。其更新过程如下：

$$
G_t = \sum_{i=1}^{t}[J'(\theta)]^2
$$

或者写成递推的形式：

$$
G_t = G_{t-1} + [J'(\theta)]^2
$$

$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{G_t + \epsilon}}\cdot J'(\theta)
$$

其中，$G_t$ 是前 t 次梯度的平方和，$\epsilon$ 是为了数值稳定性而添加的常数，通常取 $10^{-8}$。

AdaGrad 的特点是**学习率逐渐减小**，对于稀疏数据（即特征稀疏）的情况比较适用，因为对于稀疏数据，很多特征都是 0，对应的梯度也是 0，这样就不会对参数进行更新，而 AdaGrad 会根据梯度的历史信息自动调整学习率，对于出现频率较低的特征，由于其梯度较大，学习率会较小，从而使得模型参数能够得到较大的更新，而对于出现频率较高的特征，由于其梯度较小，学习率会较大，从而使得模型参数能够得到较小的更新。

### RMSProp 梯度下降

RMSProp 梯度下降法（Root Mean Square Prop，均方根传播）是在 AdaGrad 梯度下降法的基础上进行改进，**不再累加所有的历史梯度，而是只累加固定大小的窗口内的历史梯度**，这样可以避免梯度一直累加变得过大，导致学习率过小，使得模型参数无法更新。其更新过程如下：

$$
G_t = \gamma G_{t-1} + (1-\gamma)[J'(\theta)]^2
$$

$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{G_t + \epsilon}}\cdot J'(\theta)
$$

其中，$\gamma$ 是窗口大小，通常取 0.9。

### Adam 梯度下降

Adam 梯度下降法（Adaptive Moment Estimation，自适应矩估计）是在 RMSProp 梯度下降法的基础上进行改进，**不仅使用了梯度的一阶矩估计（即梯度的均值），还使用了梯度的二阶矩估计（即梯度的未中心化的方差）**，并且使用了偏差修正。其更新过程如下：

$$
m_t = \beta_1 m_{t-1} + (1-\beta_1)J'(\theta)
$$

$$
v_t = \beta_2 v_{t-1} + (1-\beta_2)[J'(\theta)]^2
$$

$$
\hat{m}_t = \frac{m_t}{1-\beta_1^t}
$$

$$
\hat{v}_t = \frac{v_t}{1-\beta_2^t}
$$

$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t + \epsilon}}\cdot \hat{m}_t
$$

其中，$\beta_1$ 和 $\beta_2$ 是衰减率，通常取 0.9 和 0.999。

Adam 梯度下降法的特点是**结合了 Momentum 和 RMSProp 的优点**，即**不仅考虑了梯度的一阶矩估计（即梯度的均值），还考虑了梯度的二阶矩估计（即梯度的未中心化的方差）**，并且使用了偏差修正，使得梯度的估计更加准确。

### AdaDelta 梯度下降

AdaDelta 梯度下降法是在 RMSProp 梯度下降法的基础上进行改进，**不再使用学习率，而是使用 RMSProp 的窗口大小作为学习率**，并且使用了偏差修正。其更新过程如下：

$$
G_t = \gamma G_{t-1} + (1-\gamma)[J'(\theta)]^2
$$

$$
\Delta \theta_t = -\frac{\sqrt{\Delta \theta_{t-1} + \epsilon}}{\sqrt{G_t + \epsilon}}\cdot J'(\theta)
$$

$$
\theta_{t+1} = \theta_t + \Delta \theta_t
$$

其中，$\gamma$ 是窗口大小，通常取 0.9。

AdaDelta 梯度下降法的特点是**不需要设置学习率**，并且使用了偏差修正，使得梯度的估计更加准确。

### Adamax 梯度下降

Adamax 梯度下降法是在 Adam 梯度下降法的基础上进行改进，**不再使用梯度的二阶矩估计（即梯度的未中心化的方差），而是使用梯度的无穷范数**，并且使用了偏差修正。其更新过程如下：

$$
m_t = \beta_1 m_{t-1} + (1-\beta_1)J'(\theta)
$$

$$
u_t = \max(\beta_2 u_{t-1}, |J'(\theta)|)
$$

$$
\theta_{t+1} = \theta_t - \frac{\alpha}{u_t}\cdot m_t
$$

其中，$\beta_1$ 和 $\beta_2$ 是衰减率，通常取 0.9 和 0.999。

Adamax 梯度下降法的特点是**不再使用梯度的二阶矩估计（即梯度的未中心化的方差），而是使用梯度的无穷范数**，并且使用了偏差修正，使得梯度的估计更加准确。

### Nadam 梯度下降

Nadam 梯度下降法是在 Adam 梯度下降法的基础上进行改进，**不再使用梯度的一阶矩估计（即梯度的均值），而是使用 NAG 的梯度估计**，并且使用了偏差修正。其更新过程如下：

$$
m_t = \beta_1 m_{t-1} + (1-\beta_1)J'(\theta)
$$

$$
v_t = \beta_2 v_{t-1} + (1-\beta_2)[J'(\theta)]^2
$$

$$
\hat{m}_t = \frac{\beta_1 m_t + (1-\beta_1)J'(\theta)}{1-\beta_1^t}
$$

$$
\hat{v}_t = \frac{\beta_2 v_t + (1-\beta_2)[J'(\theta)]^2}{1-\beta_2^t}
$$

$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t + \epsilon}}\cdot \hat{m}_t
$$

其中，$\beta_1$ 和 $\beta_2$ 是衰减率，通常取 0.9 和 0.999。

Nadam 梯度下降法的特点是**不再使用梯度的一阶矩估计（即梯度的均值），而是使用 NAG 的梯度估计**，并且使用了偏差修正，使得梯度的估计更加准确。

### AMSGrad 梯度下降

AMSGrad 梯度下降法是在 Adam 梯度下降法的基础上进行改进，**不再使用梯度的二阶矩估计（即梯度的未中心化的方差），而是使用梯度的二阶矩估计（即梯度的未中心化的方差）的上界**，并且使用了偏差修正。其更新过程如下：

$$
m_t = \beta_1 m_{t-1} + (1-\beta_1)J'(\theta)
$$

$$
v_t = \beta_2 v_{t-1} + (1-\beta_2)[J'(\theta)]^2
$$

$$
\hat{v}_t = \max(\hat{v}_{t-1}, v_t)
$$

$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t + \epsilon}}\cdot m_t
$$

其中，$\beta_1$ 和 $\beta_2$ 是衰减率，通常取 0.9 和 0.999。

AMSGrad 梯度下降法的特点是**不再使用梯度的二阶矩估计（即梯度的未中心化的方差），而是使用梯度的二阶矩估计（即梯度的未中心化的方差）的上界**，并且使用了偏差修正，使得梯度的估计更加准确。

### 优化算法的选择

在实际应用中，我们通常使用 Adam 梯度下降法，因为它的性能通常比较好，而且不需要调节学习率。

## GloVe 模型

GloVe 模型是一种用于学习词向量的模型，它的全称是 Global Vectors for Word Representation，即用于词表示的全局向量。它的目标是学习一个词的向量表示，使得这个向量表示能够表达这个词与其他词的关系，例如，词的相似性、词的类比关系等。

在介绍 GloVe 之前，我们已知有两种词嵌入方法：
1. **基于共现矩阵的方法，例如 LSA 模型**。
2. **基于神经网络的方法，例如 Word2Vec 模型**。Word2Vec 的两种模型 CBOW 和 Skip-gram 是基于神经网络的，目的是训练单词的分布式表示，只结合了上下文信息而没有全局信息。而 GloVe 模型则结合了全局语义信息和局部上下文信息，以更好地捕捉单词之间的关系。

这些模型除了在单词相似性任务上表现良好外，还展示了捕获复杂语言模式能力，但未能利用到全局共现统计数据。相比之下，GloVe 由一个**加权最小二乘模型**组成，基于全局 word-word 共现计数进行训练，从而有效地利用全局统计数据。模型生成了包含有意义的子结构的单词向量空间，在**词类比任务**上表现非常出色。

符号说明：

- $X$：word-word 共现矩阵。
- $X_{ij}$ 表示单词 i 和 j 同时出现的次数。
- $X_i=\sum_k{X_{ik}}$：单词 i 的特定大小上下文窗口中出现的所有不同单词的总次数。
- $P_{ij}=P(w_j\|w_i)=\frac{X_{ij}}{X_i}$：单词 j 出现在单词 i 的上下文中的概率。

GloVe 还根据两个单词在上下文窗口的距离 d，提出了一个衰减函数 decay=1/d 用于计算权重，也就是说距离越远的两个单词所占总计数（total count）的权重越小。

构建词向量（Word Vector）和共现矩阵（Co-ocurrence Matrix）之间的近似关系，论文的作者提出以下的公式可以近似地表达两者之间的关系：

$$
w_i^T\tilde{w}_j + b_i + \tilde{b}_j = \log X_{ij}
$$

其中，$w_i$ 和 $\tilde{w}_j$ 分别是单词 i 和 j 的词向量，$b_i$ 和 $\tilde{b}_j$ 分别是单词 i 和 j 的偏置项。

而 $w_i$ 和 $\tilde{w}_j$ 就是我们最终要求解的词向量。

Glove 利用全局统计量，以最小二乘为目标，预测单词 i 和上下文单词 j 同时出现的概率，即 $P_{ij}$。GloVe 模型的目标是通过学习单词的词向量来捕捉词语之间的语义关系，其损失函数基于共现矩阵。Glove 模型的目标函数如下：

$$
J = \sum_{i,j=1}^V f(X_{ij})(w_i^T\tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2
$$

其中，$f(X_{ij})$ 是权重函数，用于**降低高频词对损失函数的影响**，$V$ 是词表大小。权重函数的定义如下：

$$
f(X_{ij}) = \begin{cases}
(\frac{X_{ij}}{x_{max}})^\alpha & X_{ij} < x_{max} \\
1 & X_{ij} \geq x_{max}
\end{cases}
$$

这里的 $x_{max}$ 是最大共现次数，通常取 100，$\alpha$ 是一个超参数，通常取 0.75。

显然，当 $X_{ij}$ 大于 $x_{max}$ 时，权重函数 $f(X_{ij})$ 的值为 1，也就是说权重不会因为词语出现次数过多而增加，这样就避免了权重过大的问题。

由于 GloVe 的损失函数只用到了共现矩阵的元素，因此它的训练是无监督的。

在 GloVe 模型中，损失函数的 bias term 指的是在计算共现概率比率时使用的偏差项。

**下面对 GloVe 模型的损失函数进行推导**。

在计算损失函数时，GloVe 模型使用了**共现概率比率**（co-occurrence probability ratio）。这样做是因为，论文认为通过「概率的比率」而不是概率本身去学习词向量，可以更好地捕捉词语之间的关系。

共现概率比率的计算公式如下：

$$
F(w_i,w_j,\tilde{w_k}) = \frac{P_{ik}}{P_{jk}} = \frac{X_{ik}}{X_i} \cdot \frac{X_j}{X_{jk}}
$$

因为向量空间是线性结构的，所以要表达出两个概率的比例差，最简单的办法是作差，于是我们得到：

$$
F(w_i-w_j,\tilde{w_k}) = \frac{P_{ik}}{P_{jk}}
$$

这时我们发现上式的右侧是一个标量，而左侧是一个向量，于是我们把左侧转换成标量（两个向量的内积形式）：

$$
F((w_i-w_j)^T\tilde{w_k}) = \frac{P_{ik}}{P_{jk}}
$$

于是，

$$
F(w_i^T\tilde{w_k}) = P_{ik} = \frac{X_{ik}}{X_{i}}
$$

然后，我们令函数 $F$ 为 $F(x) = \exp(x)$，于是取自然对数：

$$
w_i^T\tilde{w_k} = \log P_{ik} = \log X_{ik} - \log X_i
$$

**偏差项是为了调整共现概率比率而引入的参数，用于解决非对称性建模的问题**。在训练过程中，GloVe 模型通过最小化损失函数来学习词向量和偏差项。具体而言，GloVe 模型使用随机梯度下降或其他优化算法来更新参数，其中包括词向量和偏差项。

共现矩阵 $X$ 本来是对称矩阵，然而我们发现因为等号右侧的 $\log(X_i)$ 的存在，让上式不满足对称性（symmetry）了，而且这个 $\log(X_i)$ 相对于 $k$ 是独立的，它只跟 $i$ 有关，于是我们可以针对 $w_i$ 增加一个bias term $b_i$ 把它替换掉，于是我们有：

$$
w_i^T\tilde{w_k} + b_i = \log X_{ik}
$$

同理，我们还要为 $w_k$ 增加一个 $\tilde{b}_k$（我觉得这里可能是因为 $w_i + b_i$ 所处的空间不具备线性可加性，所以要分别加偏置，不能用一个偏置来对两个词向量的差异进行建模），于是我们有：

$$
w_i^T\tilde{w_k} + b_i + \tilde{b}_k = \log X_{ik}
$$


