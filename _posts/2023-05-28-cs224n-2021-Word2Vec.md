---
layout: post
title: CS224N 2021 SVD 和 Word2Vec
date: 2023-05-28 22:31 +0000
categories: [课程笔记, CS224N]
tags: [nlp, 基础]
math: true
---

## SVD

这一节其实是要介绍「基于共现矩阵的词向量模型」。这类方法是基于共现矩阵的，可以通过 SVD 分解来降维。它有效地利用了全局的信息，但是主要用于捕获单词的相似性，对例如单词类比的任务上表现不好。在基于共现矩阵的方法中，词向量的表示通常是通过奇异值分解（Singular Value Decomposition，SVD）对共现矩阵进行降维得到的。

举个例子，假设我们有一个包含 4 个单词（"apple"、"banana"、"orange"和"grape"）的共现矩阵，其中元素 $X_{ij}$ 表示单词 i 和单词 j 的共现频率，如下所示：

```plain
      apple  banana  orange  grape
apple      0      2       1       0
banana     2      0       3       1
orange     1      3       0       2
grape      0      1       2       0
```

通过对该共现矩阵进行 SVD 分解，我们可以得到 $U$、$\Sigma$ 和$V^T$ 这三个矩阵。如果我们选择保留 2 个奇异值，则选取对角矩阵中最大的两个元素。我们可以将 U 矩阵的前两列或 V 矩阵的前两行作为单词的 2 维向量表示。

假设U矩阵的前两列为：

```plain
U = [[-0.19, 0.51],
[-0.58, -0.52],
[-0.56, 0.47],
[-0.55, -0.49]]
```

V 矩阵的前两行为：

```plain
V = [[-0.55, -0.35, -0.46, -0.60],
[-0.30, -0.61, 0.63, 0.35]]
```

然后我们**选择 U 矩阵的前 2 维作为单词的词向量表示**，即：

```plain
apple = [-0.19, 0.51]
banana = [-0.58, -0.52]
orange = [-0.56, 0.47]
grape = [-0.55, -0.49]
```

于是，原来的共现矩阵就变成了 $U[:, :2] \Sigma[:2, :2] V^T[:2, :]$。

通过这种方法，我们可以得到每个单词的向量表示，但是这种方法的缺点是**没有考虑到单词之间的局部上下文信息**，例如，"apple"和"banana"都可以作为"fruit"的上下文，但是"apple"和"banana"之间并没有直接的共现关系。因此，这种方法不能很好地捕捉单词之间的关系。

[SVD 代码](https://zhuanlan.zhihu.com/p/480389473)：

```python
# -*- coding: utf-8 -*-
import numpy as np
from numpy import linalg as la
#1. SVD分解
A= [[1,1,3,6,1],[5,1,8,4,2],[7,9,2,1,2]]
A=np.array(A)
U,s,VT = la.svd(A) 
# 为节省空间，svd输出s只有奇异值的向量
print('奇异值：',s)
# 根据奇异值向量s，生成奇异值矩阵
Sigma = np.zeros(np.shape(A))
Sigma[:len(s),:len(s)] = np.diag(s)

print("左奇异值矩阵：\n",U)
print('奇异值矩阵：\n',Sigma)
print('右奇异矩阵的转置：\n',VT)

#2.SVD重构
B = U.dot(Sigma.dot(VT))
print('重构后的矩阵B：\n', B)

print('原矩阵与重构矩阵是否相同？',np.allclose(A,B))

# 3. SVD矩阵压缩（降维）
for k in range(3,0,-1):  # 3,2,1
    # U的k列，VT的k行
    D = U[:,:k].dot(Sigma[:k,:k].dot(VT[:k,:]))
    print('k=',k,"压缩后的矩阵：\n",np.round(D,1))  # round取整数
```

## Word2Vec 模型

传统的 NLP 把词语看作离散的符号，用 one-hot 编码，向量维度大，而且不能计算相似度（one-hot 向量相互正交，余弦相似度为 0）。

分布式表示（distributed representation）也是将词语表示为向量，但是这个词向量是由经常出现在这个词附近的词给出的。维度较小，可以计算相似度。

Word2Vec 模型是一种浅层神经网络，它的输入是一个词语，输出是这个词语的词向量。Word2Vec 模型有两种：CBOW 和 Skip-gram。

**思想**：固定词汇表的每个单词由一个向量表示，文本中的每个位置 t 均有一个中心词 c 和上下文单词 o，使用 c 和 o 的词向量的相似度来计算 P(o\|c)，不断调整词向量来最大化这个概率（因为是要预测最可能的上下文词）。

**目标函数**：对每个位置 t = 1, ..., T，在大小为 m 的固定窗口内预测上下文单词，给定中心词 $w_t$，其概率函数为：

$$
L(\theta) = \prod_{t=1}^T \prod_{-m \leq j \leq m, j \neq 0} P(w_{t+j}|w_t; \theta)
$$

损失函数为：

$$
J(\theta) = -\frac{1}{T} \log L(\theta) = -\frac{1}{T} \sum_{t=1}^T \sum_{-m \leq j \leq m, j \neq 0} \log P(w_{t+j}|w_t; \theta)
$$

显然，损失函数就是对概率函数取对数，并加上负号。这是因为概率函数是要最大化的，而损失函数是要最小化的。我们通常使用梯度下降法来最小化损失函数。损失函数 $J(\theta)$ 也就是我们的目标函数。

**问题**：如何计算 $P(w_{t+j}\|w_t; \theta)$？

对一个单词 w，我们使用两个词向量来表示它，其中 $v_w$ 表示它作为中心词（center word）的词向量，$u_w$ 表示它作为上下文词（context word）的词向量。

于是，我们有：

$$
P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w\in W} \exp(u_w^T v_c)}
$$

显然，这是个 softmax 函数。对于 CBOW 模型，我们需要计算 $P(w_t\|w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m}; \theta)$，对于 Skip-gram 模型，我们需要计算 $P(w_{t+j}\|w_t; \theta)$。这两个概率都可以用 softmax 函数来计算。

![word2vec.png](https://s2.loli.net/2023/05/29/z2nTMljYudBVepA.png)

下面用 $P(u_{problems} \| v_{into})$ 来简化表示 $P(problems \| into; u_{problems}, v_{into}, \theta)$，即 $P(w_{t+j}\|w_t; \theta)$。当然，这里的 $\theta$ 是个向量，即目标函数的所有参数组成的向量（而不是只有一个参数）。

## 一些数学

怎么使用梯度下降法对 $P(o\|c)$ 进行更新？

### 向量求导基础

先掌握一点基础的向量求导法则吧：

$$
\frac{\partial x^Ta}{\partial x}=\frac{\partial a^Tx}{\partial x}=a
$$

Why? Let's go through the deduction process step by step.

We start with the expression:
$$
\frac{\partial x^Ta}{\partial x}
$$

Here, we have a vector $x$ and a vector $a$. To compute the derivative of $x^Ta$ with respect to $x$, we need to consider that $x$ and $a$ are both vectors, so we are dealing with vector calculus.

In the expression $x^Ta$, $x^T$ represents the transpose of vector $x$. Therefore, **$x^T$ is a row vector, and $a$ is a column vector**.

Now, let's expand the expression $x^Ta$ using the definition of matrix multiplication:
$$
x^Ta = \begin{bmatrix}x_1 & x_2 & \dots & x_n\end{bmatrix}\begin{bmatrix}a_1 \\ a_2 \\ \vdots \\ a_n\end{bmatrix} = x_1a_1 + x_2a_2 + \dots + x_na_n
$$

Next, we take the derivative of $x^Ta$ with respect to $x$. Since $x$ is a vector, we take the partial derivative of each term in the expanded expression:
$$
\frac{\partial}{\partial x} (x_1a_1 + x_2a_2 + \dots + x_na_n) = \begin{bmatrix}\frac{\partial}{\partial x_1} (x_1a_1) \\ \frac{\partial}{\partial x_2} (x_2a_2) \\ \vdots \\ \frac{\partial}{\partial x_n} (x_na_n)\end{bmatrix}
$$

Taking the derivative of each term with respect to its corresponding variable, we obtain:
$$
\begin{bmatrix}a_1 \\ a_2 \\ \vdots \\ a_n\end{bmatrix} = a
$$

Hence, we conclude that:
$$
\frac{\partial x^Ta}{\partial x} = a
$$

Similarly, we can deduce that:
$$
\frac{\partial a^Tx}{\partial x} = a
$$

This result holds because taking the derivative of a scalar with respect to a vector yields a vector, and the derivative of a scalar with respect to itself is 1. Therefore, the derivative of $a^Tx$ with respect to $x$ is the same as the derivative of $x^Ta$ with respect to $x$, which is $a$.

In summary, both $\frac{\partial x^Ta}{\partial x}$ and $\frac{\partial a^Tx}{\partial x}$ are equal to $a$.

### 梯度更新

To update the parameters of the expression using gradient descent, we need to compute the derivative of the objective function with respect to the parameters. In this case, we'll focus on the derivative with respect to $v_c$. Let's go through the derivation step by step.

Given the expression:
$$
P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w\in W} \exp(u_w^T v_c)}
$$

To compute the derivative of $P(o\|c)$ with respect to $v_c$, we can rewrite the expression as follows:

$$
P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w\in W} \exp(u_w^T v_c)} = \frac{e^{z_o}}{\sum_{w\in W} e^{z_w}}
$$

where $z_o = u_o^T v_c$ and $z_w = u_w^T v_c$.

Let's start by computing the derivative of $P(o\|c)$ with respect to $z_o$:

$$
\frac{\partial P(o|c)}{\partial z_o} = \frac{\partial}{\partial z_o} \left(\frac{e^{z_o}}{\sum_{w\in W} e^{z_w}}\right)
$$

By applying the quotient rule, we get:

$$
\frac{\partial P(o|c)}{\partial z_o} = \frac{e^{z_o} \sum_{w\in W} e^{z_w} - e^{z_o} e^{z_o}}{\left(\sum_{w\in W} e^{z_w}\right)^2}
$$

Simplifying this expression, we obtain:

$$
\frac{\partial P(o|c)}{\partial z_o} = \frac{e^{z_o}}{\sum_{w\in W} e^{z_w}} \cdot \frac{\sum_{w\in W} e^{z_w} - e^{z_o}}{\sum_{w\in W} e^{z_w}}
$$

We can rewrite this as:

$$
\frac{\partial P(o|c)}{\partial z_o} = P(o|c) \left(1 - \frac{e^{z_o}}{\sum_{w\in W} e^{z_w}}\right)
$$

Simplifying further, we have:

$$
\frac{\partial P(o|c)}{\partial z_o} = P(o|c) \left(1 - P(o|c)\right)
$$

Now, let's compute the derivative of $z_o$ with respect to $v_c$:

$$
\frac{\partial z_o}{\partial v_c} = \frac{\partial}{\partial v_c} (u_o^T v_c)
$$

Since $u_o^T v_c$ is a dot product, we can apply the chain rule:

$$
\frac{\partial z_o}{\partial v_c} = u_o
$$

Finally, we can compute the derivative of $P(o\|c)$ with respect to $v_c$ by applying the chain rule:

$$
\frac{\partial P(o|c)}{\partial v_c} = \frac{\partial P(o|c)}{\partial z_o} \cdot \frac{\partial z_o}{\partial v_c} = P(o|c) \left(1 - P(o|c)\right) \cdot u_o
$$

Using this derivative, we can update the parameter $v_c$ using gradient descent:

$$
v_c

 \leftarrow v_c - \eta \cdot \frac{\partial P(o|c)}{\partial v_c}
$$

where $\eta$ is the learning rate.

Note that the update process for other parameters (such as $u_o$) would involve similar derivative calculations.

## CBOW 和 Skip-gram 模型

Word2Vec 模型有**两种训练方式**：CBOW 和 Skip-gram。两种模型的输入都是一个词汇表 $V$，输出是一个 $\|V\|$ 维的向量空间，每个单词 $w \in V$ 都对应一个 $\|V\|$ 维的向量 $v_w$。

### CBOW

CBOW aims to predict a target word based on its surrounding context words. The objective function of CBOW is to maximize the probability of correctly **predicting the target word given the context words**.

给定中心词 $w_t$，其上下文单词为 $w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m}$，CBOW 模型的目标函数为：

$$
J_{CBOW}(\theta) = -\frac{1}{T} \sum_{t=1}^T \log P(w_t|w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m}; \theta)
$$

### Skip-gram

For the Skip-gram model, the objective function maximizes the average log-probability of the context words given the target word. The objective is to correctly **predict the context words surrounding a target word**. 

给定中心词 $w_t$，其上下文单词为 $w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m}$，Skip-gram 模型的目标函数为：

$$
J_{Skip-gram}(\theta) = -\frac{1}{T} \sum_{t=1}^T \sum_{-m \leq j \leq m, j \neq 0} \log P(w_{t+j}|w_t; \theta)
$$

In both CBOW and Skip-gram models, the probabilities $P(w_t \| w_{t-n}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+n})$ and $P(c \| w_t)$ are **computed using the softmax function** applied to the dot product of word embeddings.

The objective functions of CBOW and Skip-gram in Word2Vec are typically **optimized using techniques like negative sampling or hierarchical softmax** to make the training more efficient and scalable.

## 优化方法

### 负采样 (Negative Sampling)

在深度神经网络模型中，数据集中的每个句子、每对交互、每张图片都可以看作是模型的正样本，也称正例 (postive example)。在模型的训练过程中，一种常见的训练方式是同时给模型提供正例与负例 (negative example，不一定真实存在)，并构造损失函数增大正负例的区分度，从而学到数据中的信息。基于一定的策略构造与正例相对的负例的过程，称为负采样 (Negative Sampling) 。**简而言之就是构造负例**。

Negative Sampling in CBOW:
   
- Given a target word $w_t$ and its context words $w_{t-n}, w_{t-n+1}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+n}$.
- **For each training example, randomly select a few negative samples from the vocabulary**. These negative samples are **words that do not appear in the context of the target word**.
- For each positive example (target-context word pair), create a binary classification task, where the positive example is assigned a label of 1, and the negative examples are assigned a label of 0.
- **Train a binary classifier** (e.g., logistic regression or a neural network with a sigmoid activation) to distinguish between the positive and negative examples.
- The objective is to maximize the probability of classifying the positive examples as 1 and the negative examples as 0.
- During training, update the word embeddings based on the gradients obtained from the binary classifier.

By using negative sampling, CBOW focuses on **differentiating between the true context words and randomly selected negative samples**. This allows for faster training and more efficient computation compared to the full softmax approach.

为什么要负采样？因为我们只需要高质量（能够提升对比学习能力）的负样本，而不需要所有的负样本。

### 层次 Softmax (Hierarchical Softmax)

根据每个词语在文档中出现的次数构造哈夫曼树（次数少的到根结点的路径长）。现在规定左边支路用 0 编码，右边支路用 1 编码，并且“一个节点对应的编码”指的就是这个节点与其父节点之间的边上的编码（因此只可能是 0 或 1）。除叶子节点外，每一个节点都有一个 sigmoid 激活函数用于二分类，通过二分类的方式，决定走左子树还是走右子树。

设 sigmoid 函数的输入是 $x$，输出是 $y$，其函数表达式为：

$$
y = \frac{1}{1 + e^{-x}}
$$

Hierarchical softmax 利用哈夫曼树**将一个多分类问题转化为多个二分类问题**。在使用层次 softmax 时，每个单词作为输入时的词向量表示用的都是 one-hot，而层次 softmax 的目的是要学习到隐藏层的权重矩阵，也就是这些单词的 embedding 向量（多个单词所以构成了矩阵）。它的输出是一个二分类问题。

![hierarchical softmax.png](https://s2.loli.net/2023/05/29/78J9TsgnH53FeZY.png)

符号描述：

- $w$：单词。
- $X_1,X_2,\ldots,X_n$：单词 $w_i$ 的 one-hot 编码。
- $X_w$：n 个输入单词的词向量矩阵。其计算方法是：$X_w = \frac{1}{n}W_{vn}(X_1+X_2+\ldots+X_n)$，其中 $W_{vn}$ 是 n 个单词 $w_1,w_2,\ldots,w_n$ 的 embedding 向量构成的矩阵。
- $p^w$：从根节点出发到达单词 $w$（所对应的叶节点）的路径。
- $l^w$：路径 $p^w$ 中包含的节点数。
- $p_1^w,p_2^w,\ldots,p_{l^w}^w$：路径 $p^w$ 中的所有节点。其中，$p_1^w$ 是根节点，$p_{l^w}^w$ 是叶节点，即单词 $w$ 表示的节点。
- $d_2^w, d_3^w, \ldots, d_{l^w}^w\in{0,1}$：路径 $p^w$ 中除根节点以外的所有节点的编码。$d_2^w d_3^w \ldots d_{l^w}^w$ 共同构成了 $w$ 的 Huffman 编码。因此 $w$ 的 Huffman 编码长度为 $l^w$。
- $\theta_1^w, \theta_2^w, \ldots, \theta_{l^w-1}^w$：路径 $p^w$ 中除叶节点以外的所有节点的参数向量（即所有非叶子节点对应的向量）。

一棵哈夫曼树是一棵二分类树（二叉树）。

对于任意一个节点 $\theta_{j-1}^w$，选择左边或右边支路的概率：

$$
p(d_j^w|X_w,\theta_{j-1}^w)=
\left\{
\begin{array}{**lr**}
   \sigma(X^T\theta_{j-1}^w) & d_j^w=0\\
   1-\sigma(X^T\theta_{j-1}^w) & d_j^w=1\\
\end{array}
\right.
$$

因此，

$$
p(d_j^w|X_w,\theta_{j-1}^w)=[\sigma(X^T\theta_{j-1}^w)]^{1-d_j^w}\cdot[1-\sigma(X^T\theta_{j-1}^w)]^{d_j^w}
$$

用最大似然估计的思想最大化这个概率：

$$
p(w|Context(w))=\prod_{j=2}^{l_w}{[\sigma(X^T\theta_{j-1}^w)]^{1-d_j^w}\cdot[1-\sigma(X^T\theta_{j-1}^w)]^{d_j^w}}
$$

对上式取对数，得到：

$$
\begin{aligned}
   l(w)&=\log{p(w|Context(w))}\\
   &=\sum_{j=2}^{l_w}{[(1-d_j^w)\log{\sigma(X^T\theta_{j-1}^w)}+d_j^w\log{[1-\sigma(X^T\theta_{j-1}^w)]}]}
\end{aligned}
$$

而 sigmoid 函数的导数为：

$$
\frac{\partial{\sigma(x)}}{\partial{x}}=\sigma(x)(1-\sigma(x))
$$

因此，

$$
\frac{\partial{\sigma(X^T\theta_{j-1}^w)}}{\partial{X^T\theta_{j-1}^w}}=\sigma(X^T\theta_{j-1}^w)(1-\sigma(X^T\theta_{j-1}^w))
$$

对 $l(w)$ 求导，得到：

$$
\frac{\partial{l(w)}}{\partial{\theta_{j-1}^w}}=\frac{\partial{l(w)}}{\partial{\sigma(X^T\theta_{j-1}^w)}}\cdot\frac{\partial{\sigma(X^T\theta_{j-1}^w)}}{\partial{X^T\theta_{j-1}^w}}\cdot\frac{\partial{X^T\theta_{j-1}^w}}{\partial{\theta_{j-1}^w}}
$$

设 $y_j^w=\sigma(X^T\theta_{j-1}^w)$，则：

$$
\frac{\partial{l(w)}}{\partial{\theta_{j-1}^w}}=\frac{\partial{l(w)}}{\partial{y_j^w}}\cdot y_j^w(1-y_j^w) \cdot\frac{\partial{X^T\theta_{j-1}^w}}{\partial{\theta_{j-1}^w}}
$$

而 $\frac{\partial{l(w)}}{\partial{y_j^w}}$ 的计算如下：

$$
\begin{aligned}
\frac{\partial{l(w)}}{\partial{y_j^w}}
&=\frac{\partial{\sum_{j=2}^{l_w}{[(1-d_j^w)\log{\sigma(X^T\theta_{j-1}^w)}+d_j^w\log{[1-\sigma(X^T\theta_{j-1}^w)]}]}}}{\partial{y_j^w}}\\
&=\frac{\partial{\sum_{j=2}^{l_w}{[(1-d_j^w)\log{y_j^w}+d_j^w\log{(1-y_j^w)}]}}}{\partial{y_j^w}}\\
&=\sum_{j=2}^{l_w}{\frac{1-d_j^w}{y_j^w}-\frac{d_j^w}{1-y_j^w}}\\
&=\sum_{j=2}^{l_w}{\frac{1-d_j^w-y_j^w}{y_j^w(1-y_j^w)}}
\end{aligned}
$$

因此，$\frac{\partial{l(w)}}{\partial{\theta_{j-1}^w}}$ 的计算如下：

$$
\begin{aligned}
\frac{\partial{l(w)}}{\partial{\theta_{j-1}^w}}
&=\frac{\partial{l(w)}}{\partial{y_j^w}}\cdot y_j^w(1-y_j^w) \cdot\frac{\partial{X^T\theta_{j-1}^w}}{\partial{\theta_{j-1}^w}}\\
&=\sum_{j=2}^{l_w}{\frac{1-d_j^w-y_j^w}{y_j^w(1-y_j^w)}}\cdot y_j^w(1-y_j^w) \cdot X\\
&=\sum_{j=2}^{l_w}{(1-d_j^w-y_j^w) \cdot X}\\
&=\sum_{j=2}^{l_w}{[1-d_j^w-\sigma(X^T\theta_{j-1}^w)] \cdot X}
\end{aligned}
$$

同理，可以计算出：

$$
\frac{\partial{l(w)}}{\partial{X}}=\sum_{j=2}^{l_w}{[1-d_j^w-\sigma(X^T\theta_{j-1}^w)] \cdot \theta_{j-1}^w}
$$

注意，这里的 $X$ 就是上面所说的 $X_w$。

使用梯度上升更新参数：

$$
\begin{aligned}
\theta_{j-1}^w&=\theta_{j-1}^w+\alpha\cdot\frac{\partial{l(w)}}{\partial{\theta_{j-1}^w}}\\
X&=X+\alpha\cdot\frac{\partial{l(w)}}{\partial{X}}
\end{aligned}
$$

当把上面的偏导数求取出来后，就可以继续更新输入层到隐藏层的参数了，也就是说整个网络的参数就可以正常更新了，词向量矩阵 $W_{vn}$ 也就可以得到了。它与 $X_w$ 直接相关。

使用 Hierarchical softmax 训练 Word2Vec 模型，使得时间复杂度直接由 $O(V)$ 降低到了 $O(\log{V})$（这里的 $V$ 是语料库中所有的不同单词，也就是下面所说的类别）。当类别数量巨大时，Hierarchical softmax 的优势就会体现出来。设 k 是类别的数量（也就是输入的单词的数量），h 是文本特征的维度数，那么 Hierarchical softmax 的时间复杂度为 $O(h\log{k})$，而线性分类器的时间复杂度为 $O(kh)$。当 k 很大时，Hierarchical softmax 的时间复杂度就会小很多。

由于 Hierarchical softmax 是根据每个类别（即单词）出现的次数为权重来建立哈夫曼树的，所以出现次数多的类别路径就短，出现次数少的类别路径就长。因此 log 是针对类别数量（即单词数量）的。

### 总结

**Negative Sampling 和 Hierarchical softmax 的比较**：

- Negative Sampling 的优点：负采样的计算量比较小，而且对于低频词的采样效果比较好。
- Negative Sampling 的缺点：负采样的采样概率分布是固定的，而且对于高频词的采样效果不好。
- Hierarchical softmax 的优点：Hierarchical softmax 的采样概率分布是动态的，而且对于高频词的采样效果比较好。
- Hierarchical softmax 的缺点：Hierarchical softmax 的计算量比较大，而且对于低频词的采样效果不好。

**Word2Vec 的训练技巧**：

- 对于高频词，可以降低学习率，对于低频词，可以增加学习率。
- 对于高频词，可以降低采样概率，对于低频词，可以增加采样概率。
- 对于高频词，可以降低迭代次数，对于低频词，可以增加迭代次数。

**Word2Vec 的改进**：

- GloVe：GloVe 是一种基于矩阵分解的词向量模型，它的目标函数是最小化词向量的平方差和词共现矩阵的平方差之和。
- FastText：FastText 是一种基于字符级别 n-gram 的词向量模型，它的目标函数是最小化词向量的平方差和词的 n-gram 向量的平方差之和。
- ELMo：ELMo 是一种基于语言模型的词向量模型，它的目标函数是最大化语言模型的似然函数。




