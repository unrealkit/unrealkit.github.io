---
layout: post
title: CS224N 2021 Assignments 1
date: 2023-05-30 15:31 +0000
categories: [课程笔记, CS224N]
tags: [nlp, 基础]
math: true
---

SVD (Singular Value Decomposition) is a kind of generalized PCA (Principal Components Analysis) to select the top k principal components.

## 降维算法

这一节主要介绍 [PCA 降维算法](https://www.showmeai.tech/article-detail/198)。

基变换：将原始数据 $X$ 从原来的表示空间变换到新的表示空间，得到新的数据表示 $Y$。最简单的基变换就是线性变换，即 $Y = PX$，其中 $P$ 是基向量矩阵。

PCA 降维算法的目标是找到一个 $P_{r\times n}$，使得 $Y$ 的维度比 $X$ 低，但是尽可能保留 $X$ 的信息。

数学表达为：

$$
\left[
\begin{matrix}
p_1 \\
p_2 \\
\vdots \\
p_r
\end{matrix}
\right]_{r \times n}
\left[
\begin{matrix}
x_1 & x_2 & \cdots & x_m
\end{matrix}
\right]_{n \times m}
=
\left[
\begin{matrix}
p_1x_1 & p_1x_2 & \cdots & p_1x_m \\
p_2x_1 & p_2x_2 & \cdots & p_2x_m \\
\vdots & \vdots & \ddots & \vdots \\
p_rx_1 & p_rx_2 & \cdots & p_rx_m
\end{matrix}
\right]_{r \times m}
$$

其中 $p_i$ 是行向量，表示第 i 个基；$x_j$ 是列向量，表示第 j 条数据。

当 r < n 时，$Y$ 的维度比 $X$ 低，达到了降维的目的。即 $X\in R^{n\times m}\to Y\in R^{r\times m}$。

降维的目的是希望**压缩数据但信息损失最少**，也就是说，我们**希望投影后的数据尽可能分散开**。在数学上，这种分散程度我们用「方差」来表达，方差越大，数据越分散。

方差 $Var(X)$ 的计算公式为：

$$
Var(X) = \frac{1}{m}\sum_{i=1}^m(x_i-\mu)^2
$$

其中 $\mu$ 是 $X$ 的均值。

为了后面方便操作，这里对数据去中心化，即 $X$ 的每一维减去该维的均值，得到 $X'$：

$$
X' = X - \mu
$$

$$
Var(X') = \frac{1}{m}\sum_{i=1}^m(x_i-\mu)^2 = \frac{1}{m}\sum_{i=1}^mx_i^2
$$

举一个例子，假设有 5 个样本数据，每个数据都各有 2 维特征。设 $x_1=[1,1]^T$，$x_2=[2,2]^T$，$x_3=[3,3]^T$，$x_4=[4,4]^T$，$x_5=[5,5]^T$。则 $X=[x_1,x_2,x_3,x_4,x_5]$，$X$ 的均值为 $\mu=[3,3]^T$，$X'$ 为：

$$
X' = X - \mu = \left[
\begin{matrix}
-2 & -1 & 0 & 1 & 2 \\
-2 & -1 & 0 & 1 & 2
\end{matrix}
\right]
$$

然后对每一行（一行就是同一个特征）求方差。

协方差（Covariance）矩阵 $C$ 的计算公式为：

$$
Cov(a,b) = \frac{1}{m}\sum_{i=1}^m(a_i-\mu_a)(b_i-\mu_b)
$$

当 $Cov(a,b)=0$ 时，变量 $a$ 和 $b$ 是相互独立的。

当 $a=b$ 时，$Cov(a,b)$ 就是方差 $Var(a)$。

对二维随机变量：$$x_i=\left[\begin{matrix}a\\b\end{matrix}\right]$$，其协方差矩阵为：

$$
C=\left[\begin{matrix}Cov(a,a) & Cov(a,b) \\ Cov(b,a) & Cov(b,b)\end{matrix}\right]=\left[\begin{matrix}Var(a) & Cov(a,b) \\ Cov(b,a) & Var(b)\end{matrix}\right]
$$


对 n 维随机变量 $$x_i=\left[\begin{matrix}x_1 \\ x_2 \\ \vdots \\ x_n\end{matrix}\right]$$，协方差矩阵为：

$$
C=\left[\begin{matrix}Var(x_1) & Cov(x_1,x_2) & \dots & Cov(x_1,x_n) \\Cov(x_2,x_1) & Var(x_2) & \dots & Cov(x_2,x_n) \\ \vdots & \vdots & \ddots & \vdots \\ Cov(x_n,x_1) & Cov(x_n,x_2) & \cdots & Var(x_n) \\ \end{matrix}\right]
$$

注意，协方差矩阵其实是在求特征之间的协方差，因此，如果有 m 个样本，每个样本有 2 维特征，即 $$X=[x_1,x_2,\dots,x_m]=\left[\begin{matrix}a_1&a_2&\dots&a_m\\b_1&b_2&\dots&b_m\end{matrix}\right]$$，则协方差矩阵为：

$$
\begin{aligned}
    C&=\frac{1}{m}XX^T=\frac{1}{m}\left[\begin{matrix}a_1&a_2&\dots&a_m\\b_1&b_2&\dots&b_m\end{matrix}\right]\left[\begin{matrix}a_1&b_1 \\ a_2&b_2 \\ \vdots&\vdots \\ a_m&b_m \end{matrix}\right] \\
    &=\left[\begin{matrix}
        \frac{1}{m}\sum_{i=1}^ma_i^2 & \frac{1}{m}\sum_{i=1}^ma_ib_i \\
        \frac{1}{m}\sum_{i=1}^ma_ib_i & \frac{1}{m}\sum_{i=1}^mb_i^2
    \end{matrix}\right]
\end{aligned}
$$

从 2 维归纳到 n 维，可以得到：假设我们有 m 个 n 维数据记录，将其按列排成 $n\times m$ 的矩阵 $X$，设 $C=\frac{1}{m}XX^T$，则 $C$ 是一个对称矩阵，$C_{ij}$ 表示特征 i 和特征 j 的协方差。

那么怎么降维呢？

之前我们不是想找到一个基 $P$ 使得 $Y_{r\times m}=P_{r\times n}X_{n\times m}$，然后通过 $r<n$ 降维吗？设 $X$ 的协方差矩阵为 $C$，$Y$ 的协方差矩阵为 $D$，且 $Y=PX$。那么，

$$
\begin{aligned}
    D&=\frac{1}{m}YY^T \\
    &=\frac{1}{m}(PX)(PX)^T \\
    &=\frac{1}{m}PXX^TP^T \\
    &=\frac{1}{m}PCP^T
\end{aligned}
$$

因此我们发现，要找的 $P$ 不是别的，而是能让原始协方差矩阵对角化的 $P$。这样，我们就可以通过对角化的 $P$ 得到 $Y$ 的协方差矩阵 $D$，然后取前 $r$ 个特征向量组成 $P$，就可以得到降维后的 $Y$。

根据相似对角化的知识，我们知道，$P$ 其实就是 $C$ 的特征向量组成的矩阵，$D$ 是 $C$ 的特征值组成的对角矩阵。因此，我们只需要对 $C$ 进行特征值分解，然后取**前 $r$ 个特征值（按由大到小排序）**对应的特征向量组成 $P$，就可以得到降维后的 $Y$。

